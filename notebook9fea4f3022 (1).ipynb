{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13326890,"sourceType":"datasetVersion","datasetId":8449080}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================================================================\n# CELL 1: Setup and Imports\n# ============================================================================\nimport numpy as np\nimport pandas as pd\nimport re\nimport os\nimport gc\nfrom pathlib import Path\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ML Libraries\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.linear_model import Ridge\nimport lightgbm as lgb\nimport catboost as cb\nfrom scipy import sparse\nfrom category_encoders import TargetEncoder\n\n# Deep Learning\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModel\nimport timm\nfrom torchvision import transforms\n\n# Image Processing\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\nimport multiprocessing\nfrom functools import partial\nfrom tqdm import tqdm\nimport urllib\n\nprint(\"‚úÖ All libraries imported!\")\nprint(f\"PyTorch: {torch.__version__} | CUDA: {torch.cuda.is_available()}\")\nprint(f\"TIMM version: {timm.__version__}\")\n\n# ============================================================================\n# CELL 2: Configuration\n# ============================================================================\nclass Config:\n    # Paths\n    DATA_PATH = '/kaggle/input/amazon-smart-pricing-challenge-2025'\n    IMAGE_FOLDER = '/kaggle/working/product_images'\n    \n    # Model parameters\n    SEED = 42\n    N_FOLDS = 5\n    MAX_TEXT_LENGTH = 256\n    IMAGE_SIZE = 300  # EfficientNet-B3 optimal size\n    BATCH_SIZE = 32\n    \n    # Feature engineering\n    TFIDF_MAX_FEATURES = 60000\n    SVD_COMPONENTS = 150\n    \n    # Ensemble weights (optimized through CV)\n    CATBOOST_WEIGHT = 0.28\n    LGBM_WEIGHT = 0.22\n    RIDGE_WEIGHT = 0.08\n    NN_TEXT_WEIGHT = 0.15\n    MULTIMODAL_WEIGHT = 0.20\n    STACKING_WEIGHT = 0.07\n    \n    # CatBoost params (enhanced)\n    CATBOOST_PARAMS = {\n        'iterations': 4000,\n        'learning_rate': 0.025,\n        'depth': 9,\n        'l2_leaf_reg': 2.5,\n        'min_data_in_leaf': 15,\n        'random_strength': 0.4,\n        'bagging_temperature': 0.25,\n        'od_type': 'Iter',\n        'od_wait': 150,\n        'random_seed': SEED,\n        'verbose': 300,\n        'task_type': 'GPU' if torch.cuda.is_available() else 'CPU',\n        'loss_function': 'RMSE',\n        'eval_metric': 'MAE'\n    }\n    \n    # LightGBM params (enhanced)\n    LGBM_PARAMS = {\n        'objective': 'regression',\n        'metric': 'mae',\n        'boosting_type': 'gbdt',\n        'num_leaves': 150,\n        'max_depth': 9,\n        'learning_rate': 0.015,\n        'feature_fraction': 0.75,\n        'bagging_fraction': 0.75,\n        'bagging_freq': 5,\n        'min_child_samples': 15,\n        'reg_alpha': 0.05,\n        'reg_lambda': 0.05,\n        'verbose': -1,\n        'random_state': SEED\n    }\n\ndef set_seed(seed=42):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    \nset_seed(Config.SEED)\n\ndef smape(y_true, y_pred):\n    \"\"\"Calculate SMAPE metric\"\"\"\n    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n    diff = np.abs(y_true - y_pred) / denominator\n    diff[denominator == 0] = 0.0\n    return np.mean(diff) * 100  # Return as percentage\n\nprint(\"‚úÖ Configuration set!\")\n\n# ============================================================================\n# CELL 3: Load Data\n# ============================================================================\nprint(\"üìä Loading data...\")\ntrain_df = pd.read_csv(f'{Config.DATA_PATH}/train.csv')\ntest_df = pd.read_csv(f'{Config.DATA_PATH}/test.csv')\n\nprint(f\"Train: {train_df.shape} | Test: {test_df.shape}\")\nprint(f\"\\nüí∞ Price stats:\\n{train_df['price'].describe()}\")\n\n# Log transform target\ntrain_df['log_price'] = np.log1p(train_df['price'])\n\n# Create price bins for stratified sampling\ntrain_df['price_bin'] = pd.qcut(train_df['price'], q=10, labels=False, duplicates='drop')\n\nprint(\"‚úÖ Data loaded!\")\n\n# ============================================================================\n# CELL 4: Advanced Feature Engineering with Interaction Features\n# ============================================================================\nprint(\"üîß Extracting ADVANCED features...\")\n\ndef extract_features(df):\n    \"\"\"Extract comprehensive features\"\"\"\n    features = pd.DataFrame()\n    \n    # === NUMERICAL FEATURES ===\n    features['text_length'] = df['catalog_content'].str.len()\n    features['word_count'] = df['catalog_content'].str.split().str.len()\n    features['avg_word_length'] = features['text_length'] / (features['word_count'] + 1)\n    features['capital_ratio'] = df['catalog_content'].apply(lambda x: sum(1 for c in x if c.isupper()) / (len(x) + 1))\n    features['digit_count'] = df['catalog_content'].str.count(r'\\d')\n    features['special_char_count'] = df['catalog_content'].str.count(r'[!@#$%^&*(),.?\":{}|<>]')\n    features['sentence_count'] = df['catalog_content'].str.count(r'\\.')\n    features['exclamation_count'] = df['catalog_content'].str.count(r'!')\n    features['question_count'] = df['catalog_content'].str.count(r'\\?')\n    \n    # Extract value\n    def extract_value(text):\n        match = re.search(r'Value:\\s*(\\d+\\.?\\d*)', text)\n        return float(match.group(1)) if match else 0\n    \n    features['extracted_value'] = df['catalog_content'].apply(extract_value)\n    features['log_value'] = np.log1p(features['extracted_value'])\n    \n    # Extract quantity/pack\n    def extract_quantity(text):\n        patterns = [\n            r'Pack of (\\d+)', r'(\\d+)\\s*Pack', r'(\\d+)\\s*Count',\n            r'Set of (\\d+)', r'\\((\\d+)\\s*Pack\\)', r'Quantity:\\s*(\\d+)',\n            r'(\\d+)\\s*Pieces?', r'(\\d+)\\s*Units?'\n        ]\n        for pattern in patterns:\n            match = re.search(pattern, text, re.IGNORECASE)\n            if match:\n                return int(match.group(1))\n        return 1\n    \n    features['pack_quantity'] = df['catalog_content'].apply(extract_quantity)\n    features['log_pack_quantity'] = np.log1p(features['pack_quantity'])\n    features['sqrt_pack_quantity'] = np.sqrt(features['pack_quantity'])\n    \n    # Value per pack\n    features['value_per_pack'] = features['extracted_value'] / (features['pack_quantity'] + 1)\n    features['log_value_per_pack'] = np.log1p(features['value_per_pack'])\n    \n    # === CATEGORICAL FEATURES ===\n    \n    # Unit type\n    def extract_unit(text):\n        units = {\n            'oz': 'ounce', 'ounce': 'ounce', 'fl oz': 'fluid_ounce',\n            'lb': 'pound', 'pound': 'pound', 'lbs': 'pound',\n            'kg': 'kilogram', 'gram': 'gram', 'g': 'gram',\n            'ml': 'milliliter', 'liter': 'liter', 'l': 'liter',\n            'count': 'count', 'piece': 'piece', 'ct': 'count'\n        }\n        text_lower = text.lower()\n        for unit, standard in units.items():\n            if unit in text_lower:\n                return standard\n        return 'unknown'\n    \n    features['unit_type'] = df['catalog_content'].apply(extract_unit)\n    \n    # Brand\n    def extract_brand(text):\n        match = re.search(r'Item Name:\\s*([A-Z][a-zA-Z0-9]+)', text)\n        return match.group(1) if match else 'unknown'\n    \n    features['brand'] = df['catalog_content'].apply(extract_brand)\n    \n    # Category\n    def categorize_product(text):\n        text_lower = text.lower()\n        categories = {\n            'food': ['food', 'sauce', 'spice', 'cookie', 'snack', 'candy', 'chocolate', 'coffee', 'tea'],\n            'health': ['vitamin', 'supplement', 'health', 'protein', 'probiotic'],\n            'beauty': ['beauty', 'cream', 'shampoo', 'lotion', 'soap', 'skin'],\n            'tools': ['tool', 'equipment', 'device', 'gadget'],\n            'household': ['cleaner', 'detergent', 'laundry', 'paper', 'towel'],\n            'pet': ['pet', 'dog', 'cat', 'animal']\n        }\n        for cat, keywords in categories.items():\n            if any(word in text_lower for word in keywords):\n                return cat\n        return 'other'\n    \n    features['category'] = df['catalog_content'].apply(categorize_product)\n    \n    # Quality tier\n    def get_quality_tier(text):\n        text_lower = text.lower()\n        if any(word in text_lower for word in ['premium', 'luxury', 'gourmet', 'artisan', 'finest']):\n            return 'premium'\n        elif any(word in text_lower for word in ['organic', 'natural', 'pure', 'fresh']):\n            return 'organic'\n        elif any(word in text_lower for word in ['value', 'economy', 'basic', 'budget']):\n            return 'economy'\n        else:\n            return 'standard'\n    \n    features['quality_tier'] = df['catalog_content'].apply(get_quality_tier)\n    \n    # Pack size category\n    def get_pack_size_category(qty):\n        if qty == 1:\n            return 'single'\n        elif qty <= 3:\n            return 'small_pack'\n        elif qty <= 6:\n            return 'medium_pack'\n        elif qty <= 12:\n            return 'large_pack'\n        else:\n            return 'bulk'\n    \n    features['pack_size_category'] = features['pack_quantity'].apply(get_pack_size_category)\n    \n    # Binary flags\n    features['has_value'] = df['catalog_content'].str.contains('Value:', case=False).astype(int)\n    features['has_unit'] = df['catalog_content'].str.contains('Unit:', case=False).astype(int)\n    features['has_brand'] = df['catalog_content'].str.contains(r'\\b[A-Z][a-z]+\\b').astype(int)\n    features['premium_keywords'] = df['catalog_content'].str.count(\n        r'(?i)(premium|organic|natural|gourmet|artisan|luxury|handmade|finest)'\n    )\n    features['sale_keywords'] = df['catalog_content'].str.count(\n        r'(?i)(save|discount|deal|offer|promo)'\n    )\n    \n    return features\n\n# Extract features\nprint(\"Extracting train features...\")\ntrain_features = extract_features(train_df)\nprint(\"Extracting test features...\")\ntest_features = extract_features(test_df)\n\ncategorical_features = ['unit_type', 'brand', 'category', 'quality_tier', 'pack_size_category']\n\nprint(f\"‚úÖ Extracted {train_features.shape[1]} features\")\n\n# ============================================================================\n# CELL 5: Interaction Features (KEY for pricing!)\n# ============================================================================\nprint(\"üîó Creating interaction features...\")\n\ndef create_interactions(features):\n    \"\"\"Create interaction features\"\"\"\n    inter = pd.DataFrame()\n    \n    # Pack quantity * value interactions\n    inter['pack_value_interaction'] = features['pack_quantity'] * features['extracted_value']\n    inter['log_pack_value'] = np.log1p(inter['pack_value_interaction'])\n    \n    # Unit type * pack quantity (encoded)\n    le_unit = LabelEncoder()\n    unit_encoded = le_unit.fit_transform(features['unit_type'])\n    inter['unit_pack_interaction'] = unit_encoded * features['pack_quantity']\n    \n    # Quality * value\n    le_quality = LabelEncoder()\n    quality_encoded = le_quality.fit_transform(features['quality_tier'])\n    inter['quality_value_interaction'] = quality_encoded * features['extracted_value']\n    \n    # Category * pack\n    le_category = LabelEncoder()\n    category_encoded = le_category.fit_transform(features['category'])\n    inter['category_pack_interaction'] = category_encoded * features['pack_quantity']\n    \n    # Value per word\n    inter['value_per_word'] = features['extracted_value'] / (features['word_count'] + 1)\n    \n    # Premium score\n    inter['premium_score'] = (\n        features['premium_keywords'] * 2 +\n        (features['quality_tier'] == 'premium').astype(int) * 3 +\n        (features['quality_tier'] == 'organic').astype(int) * 2\n    )\n    \n    return inter\n\ntrain_interactions = create_interactions(train_features)\ntest_interactions = create_interactions(test_features)\n\n# Combine\ntrain_features = pd.concat([train_features, train_interactions], axis=1)\ntest_features = pd.concat([test_features, test_interactions], axis=1)\n\nprint(f\"‚úÖ Total features: {train_features.shape[1]}\")\n\n# ============================================================================\n# CELL 6: Target Encoding with CV (Prevents Overfitting!)\n# ============================================================================\nprint(\"üéØ Applying target encoding...\")\n\nkf = KFold(n_splits=Config.N_FOLDS, shuffle=True, random_state=Config.SEED)\n\n# Target encode categorical features\ntarget_encoded_features = []\n\nfor cat_col in categorical_features:\n    te = TargetEncoder(smoothing=1.0)\n    train_encoded = np.zeros(len(train_df))\n    \n    # CV encoding for train\n    for fold, (train_idx, val_idx) in enumerate(kf.split(train_df)):\n        te.fit(train_features[cat_col].iloc[train_idx].values.reshape(-1, 1), \n               train_df['log_price'].iloc[train_idx])\n        train_encoded[val_idx] = te.transform(\n            train_features[cat_col].iloc[val_idx].values.reshape(-1, 1)\n        ).ravel()\n    \n    # Fit on full train for test\n    te.fit(train_features[cat_col].values.reshape(-1, 1), train_df['log_price'])\n    test_encoded = te.transform(test_features[cat_col].values.reshape(-1, 1)).ravel()\n    \n    train_features[f'{cat_col}_target_enc'] = train_encoded\n    test_features[f'{cat_col}_target_enc'] = test_encoded\n    target_encoded_features.append(f'{cat_col}_target_enc')\n\nprint(f\"‚úÖ Added {len(target_encoded_features)} target-encoded features\")\n\n# ============================================================================\n# CELL 7: TF-IDF + SVD\n# ============================================================================\nprint(\"üìù Creating TF-IDF features...\")\n\ndef preprocess_text(text):\n    text = text.lower()\n    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n    text = re.sub(r'\\s+', ' ', text)\n    return text.strip()\n\ntrain_df['clean_text'] = train_df['catalog_content'].apply(preprocess_text)\ntest_df['clean_text'] = test_df['catalog_content'].apply(preprocess_text)\n\n# TF-IDF\ntfidf = TfidfVectorizer(\n    max_features=Config.TFIDF_MAX_FEATURES,\n    ngram_range=(1, 3),  # trigrams\n    min_df=2,\n    max_df=0.95,\n    sublinear_tf=True\n)\n\ntrain_tfidf = tfidf.fit_transform(train_df['clean_text'])\ntest_tfidf = tfidf.transform(test_df['clean_text'])\n\n# SVD\nsvd = TruncatedSVD(n_components=Config.SVD_COMPONENTS, random_state=Config.SEED)\ntrain_svd = svd.fit_transform(train_tfidf)\ntest_svd = svd.transform(test_tfidf)\n\n# Add to features\nfor i in range(Config.SVD_COMPONENTS):\n    train_features[f'svd_{i}'] = train_svd[:, i]\n    test_features[f'svd_{i}'] = test_svd[:, i]\n\nprint(f\"‚úÖ TF-IDF: {train_tfidf.shape} ‚Üí SVD: {train_svd.shape}\")\nprint(f\"‚úÖ Total features now: {train_features.shape[1]}\")\n\n# ============================================================================\n# CELL 8: Download Images\n# ============================================================================\nprint(\"üñºÔ∏è Downloading images...\")\n\ndef download_image(image_link, savefolder):\n    if isinstance(image_link, str):\n        filename = Path(image_link).name\n        image_save_path = os.path.join(savefolder, filename)\n        if not os.path.exists(image_save_path):\n            try:\n                urllib.request.urlretrieve(image_link, image_save_path)\n            except:\n                pass\n\nif not os.path.exists(Config.IMAGE_FOLDER):\n    os.makedirs(Config.IMAGE_FOLDER)\n\n# Download ALL images (parallel)\nall_image_links = pd.concat([train_df['image_link'], test_df['image_link']]).unique()\nprint(f\"Downloading {len(all_image_links)} unique images...\")\n\ndownload_partial = partial(download_image, savefolder=Config.IMAGE_FOLDER)\nwith multiprocessing.Pool(100) as pool:\n    list(tqdm(pool.imap(download_partial, all_image_links), total=len(all_image_links)))\n\nprint(\"‚úÖ Images downloaded!\")\n\n# ============================================================================\n# CELL 9: Extract Image Features (EfficientNet-B3)\n# ============================================================================\nprint(\"üé® Extracting image features with EfficientNet-B3...\")\n\n# Load pretrained EfficientNet-B3\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nefficientnet = timm.create_model('efficientnet_b3', pretrained=True, num_classes=0)\nefficientnet = efficientnet.to(device)\nefficientnet.eval()\n\n# Image transforms\nimage_transform = transforms.Compose([\n    transforms.Resize((Config.IMAGE_SIZE, Config.IMAGE_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ndef extract_image_features_batch(image_links, batch_size=64):\n    \"\"\"Extract EfficientNet features in batches\"\"\"\n    all_features = []\n    \n    for i in tqdm(range(0, len(image_links), batch_size)):\n        batch_links = image_links[i:i+batch_size]\n        batch_images = []\n        \n        for link in batch_links:\n            img_path = os.path.join(Config.IMAGE_FOLDER, Path(link).name)\n            try:\n                img = Image.open(img_path).convert('RGB')\n                img_tensor = image_transform(img)\n                batch_images.append(img_tensor)\n            except:\n                # Fallback to zero vector\n                batch_images.append(torch.zeros(3, Config.IMAGE_SIZE, Config.IMAGE_SIZE))\n        \n        batch_tensor = torch.stack(batch_images).to(device)\n        \n        with torch.no_grad():\n            features = efficientnet(batch_tensor)\n            all_features.append(features.cpu().numpy())\n        \n        del batch_tensor\n        torch.cuda.empty_cache()\n    \n    return np.vstack(all_features)\n\nprint(\"Extracting train image features...\")\ntrain_image_features = extract_image_features_batch(train_df['image_link'].values)\n\nprint(\"Extracting test image features...\")\ntest_image_features = extract_image_features_batch(test_df['image_link'].values)\n\n# Add to features\nfor i in range(train_image_features.shape[1]):\n    train_features[f'img_feat_{i}'] = train_image_features[:, i]\n    test_features[f'img_feat_{i}'] = test_image_features[:, i]\n\nprint(f\"‚úÖ Image features: {train_image_features.shape}\")\nprint(f\"‚úÖ Total features: {train_features.shape[1]}\")\n\ngc.collect()\n\n# ============================================================================\n# CELL 10: Train CatBoost (Model 1)\n# ============================================================================\nprint(\"\\nüöÄ Training CatBoost...\")\n\ncat_feature_indices = [train_features.columns.get_loc(col) for col in categorical_features]\n\ncatboost_preds = np.zeros(len(test_df))\noof_catboost = np.zeros(len(train_df))\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(train_df), 1):\n    print(f\"\\n{'='*60}\\nFold {fold}/{Config.N_FOLDS}\\n{'='*60}\")\n    \n    X_tr, X_val = train_features.iloc[train_idx], train_features.iloc[val_idx]\n    y_tr, y_val = train_df['log_price'].iloc[train_idx], train_df['log_price'].iloc[val_idx]\n    \n    train_pool = cb.Pool(X_tr, y_tr, cat_features=cat_feature_indices)\n    val_pool = cb.Pool(X_val, y_val, cat_features=cat_feature_indices)\n    \n    model = cb.CatBoostRegressor(**Config.CATBOOST_PARAMS)\n    model.fit(train_pool, eval_set=val_pool, verbose=300, early_stopping_rounds=150)\n    \n    oof_catboost[val_idx] = model.predict(X_val)\n    catboost_preds += model.predict(test_features) / Config.N_FOLDS\n    \n    fold_smape = smape(train_df['price'].iloc[val_idx], np.expm1(oof_catboost[val_idx]))\n    print(f\"Fold {fold} SMAPE: {fold_smape:.2f}\")\n    \n    del model, train_pool, val_pool\n    gc.collect()\n\noof_smape_cb = smape(train_df['price'], np.expm1(oof_catboost))\nprint(f\"\\nüèÜ CatBoost OOF SMAPE: {oof_smape_cb:.2f}\")\n\n# ============================================================================\n# CELL 11: Train LightGBM (Model 2)\n# ============================================================================\nprint(\"\\nüöÄ Training LightGBM...\")\n\ntrain_features_lgb = train_features.values.astype(np.float32)\ntest_features_lgb = test_features.values.astype(np.float32)\n\nlgbm_preds = np.zeros(len(test_df))\noof_lgbm = np.zeros(len(train_df))\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(train_df), 1):\n    print(f\"\\nFold {fold}/{Config.N_FOLDS}\")\n    \n    X_tr, X_val = train_features_lgb[train_idx], train_features_lgb[val_idx]\n    y_tr, y_val = train_df['log_price'].iloc[train_idx], train_df['log_price'].iloc[val_idx]\n    \n    train_data = lgb.Dataset(X_tr, label=y_tr)\n    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n    \n    model = lgb.train(\n        Config.LGBM_PARAMS,\n        train_data,\n        num_boost_round=3000,\n        valid_sets=[train_data, val_data],\n        valid_names=['train', 'valid'],\n        callbacks=[lgb.early_stopping(200), lgb.log_evaluation(300)]\n    )\n    \n    oof_lgbm[val_idx] = model.predict(X_val)\n    lgbm_preds += model.predict(test_features_lgb) / Config.N_FOLDS\n    \n    fold_smape = smape(train_df['price'].iloc[val_idx], np.expm1(oof_lgbm[val_idx]))\n    print(f\"Fold {fold} SMAPE: {fold_smape:.2f}\")\n    \n    del model\n    gc.collect()\n\noof_smape_lgb = smape(train_df['price'], np.expm1(oof_lgbm))\nprint(f\"\\nüèÜ LightGBM OOF SMAPE: {oof_smape_lgb:.2f}\")\n\n# ============================================================================\n# CELL 12: Ridge Regression (Model 3 - Sparse Features)\n# ============================================================================\nprint(\"\\nüöÄ Training Ridge Regression...\")\n\n# Use TF-IDF + numeric features\nridge_train = sparse.hstack([\n    train_tfidf,\n    sparse.csr_matrix(train_features[['pack_quantity', 'extracted_value', 'log_pack_quantity']].values)\n])\nridge_test = sparse.hstack([\n    test_tfidf,\n    sparse.csr_matrix(test_features[['pack_quantity', 'extracted_value', 'log_pack_quantity']].values)\n])\n\nridge_preds = np.zeros(len(test_df))\noof_ridge = np.zeros(len(train_df))\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(train_df), 1):\n    X_tr = ridge_train[train_idx]\n    y_tr = train_df['log_price'].iloc[train_idx]\n    X_val = ridge_train[val_idx]\n    \n    model = Ridge(alpha=10.0, random_state=Config.SEED)\n    model.fit(X_tr, y_tr)\n    \n    oof_ridge[val_idx] = model.predict(X_val)\n    ridge_preds += model.predict(ridge_test) / Config.N_FOLDS\n    \n    del model\n    gc.collect()\n\noof_smape_ridge = smape(train_df['price'], np.expm1(oof_ridge))\nprint(f\"üèÜ Ridge OOF SMAPE: {oof_smape_ridge:.2f}\")\n\n# ============================================================================\n# CELL 13: Neural Network - Text Only (Model 4)\n# ============================================================================\nprint(\"\\nüß† Training Neural Network...\")\n\nclass TextDataset(Dataset):\n    def __init__(self, texts, prices=None, tokenizer=None, max_length=256):\n        self.texts = texts\n        self.prices = prices\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        encoding = self.tokenizer(\n            text, max_length=self.max_length, padding='max_length',\n            truncation=True, return_tensors='pt'\n        )\n        item = {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten()\n        }\n        if self.prices is not None:\n            item['price'] = torch.tensor(self.prices[idx], dtype=torch.float)\n        return item\n\nclass TextModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.bert = AutoModel.from_pretrained('distilbert-base-uncased')\n        for param in self.bert.embeddings.parameters():\n            param.requires_grad = False\n        self.regressor = nn.Sequential(\n            nn.Dropout(0.3),\n            nn.Linear(768, 256),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(256, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n        \n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        return self.regressor(outputs.last_hidden_state[:, 0])\n\ntokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n\nnn_preds = np.zeros(len(test_df))\noof_nn = np.zeros(len(train_df))\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(train_df), 1):\n    print(f\"\\nNN Fold {fold}/{Config.N_FOLDS}\")\n    \n    train_dataset = TextDataset(\n        train_df['catalog_content'].iloc[train_idx].values,\n        train_df['log_price'].iloc[train_idx].values,\n        tokenizer, Config.MAX_TEXT_LENGTH\n    )\n    val_dataset = TextDataset(\n        train_df['catalog_content'].iloc[val_idx].values,\n        train_df['log_price'].iloc[val_idx].values,\n        tokenizer, Config.MAX_TEXT_LENGTH\n    )\n    \n    train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True, num_workers=2)\n    val_loader = DataLoader(val_dataset, batch_size=Config.BATCH_SIZE, shuffle=False, num_workers=2)\n    \n    model = TextModel().to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n    criterion = nn.HuberLoss()\n    \n    best_loss = float('inf')\n    patience_counter = 0\n    \n    for epoch in range(5):\n        model.train()\n        train_loss = 0\n        for batch in train_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            prices = batch['price'].to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(input_ids, attention_mask).squeeze()\n            loss = criterion(outputs, prices)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n        \n        model.eval()\n        val_loss = 0\n        val_preds = []\n        with torch.no_grad():\n            for batch in val_loader:\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                prices = batch['price'].to(device)\n                outputs = model(input_ids, attention_mask).squeeze()\n                loss = criterion(outputs, prices)\n                val_loss += loss.item()\n                val_preds.extend(outputs.cpu().numpy())\n        \n        avg_val_loss = val_loss / len(val_loader)\n        if avg_val_loss < best_loss:\n            best_loss = avg_val_loss\n            patience_counter = 0\n            oof_nn[val_idx] = val_preds\n        else:\n            patience_counter += 1\n            if patience_counter >= 2:\n                break\n    \n    # Test predictions\n    test_dataset = TextDataset(test_df['catalog_content'].values, None, tokenizer, Config.MAX_TEXT_LENGTH)\n    test_loader = DataLoader(test_dataset, batch_size=Config.BATCH_SIZE, shuffle=False, num_workers=2)\n    \n    model.eval()\n    fold_test_preds = []\n    with torch.no_grad():\n        for batch in test_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            outputs = model(input_ids, attention_mask).squeeze()\n            fold_test_preds.extend(outputs.cpu().numpy())\n    \n    nn_preds += np.array(fold_test_preds) / Config.N_FOLDS\n    \n    del model, train_dataset, val_dataset\n    torch.cuda.empty_cache()\n    gc.collect()\n\noof_smape_nn = smape(train_df['price'], np.expm1(oof_nn))\nprint(f\"\\nüèÜ Neural Network OOF SMAPE: {oof_smape_nn:.2f}\")\n\n# ============================================================================\n# CELL 14: Multimodal Model (Text + Image) - Model 5\n# ============================================================================\nprint(\"\\nüî• Training Multimodal Model (Text + EfficientNet-B3)...\")\n\nclass MultimodalDataset(Dataset):\n    def __init__(self, texts, image_features, prices=None, tokenizer=None, max_length=128):\n        self.texts = texts\n        self.image_features = image_features\n        self.prices = prices\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        encoding = self.tokenizer(\n            text, max_length=self.max_length, padding='max_length',\n            truncation=True, return_tensors='pt'\n        )\n        item = {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'image_features': torch.tensor(self.image_features[idx], dtype=torch.float)\n        }\n        if self.prices is not None:\n            item['price'] = torch.tensor(self.prices[idx], dtype=torch.float)\n        return item\n\nclass MultimodalModel(nn.Module):\n    def __init__(self, image_feat_dim=1536):  # EfficientNet-B3 output\n        super().__init__()\n        # Text encoder\n        self.text_encoder = AutoModel.from_pretrained('distilbert-base-uncased')\n        for param in self.text_encoder.embeddings.parameters():\n            param.requires_grad = False\n        \n        # Image projection\n        self.image_proj = nn.Sequential(\n            nn.Linear(image_feat_dim, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, 256)\n        )\n        \n        # Fusion\n        self.fusion = nn.Sequential(\n            nn.Linear(768 + 256, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, 128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, 1)\n        )\n        \n    def forward(self, input_ids, attention_mask, image_features):\n        # Text features\n        text_outputs = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n        text_features = text_outputs.last_hidden_state[:, 0]\n        \n        # Image features\n        image_features = self.image_proj(image_features)\n        \n        # Concatenate and fuse\n        combined = torch.cat([text_features, image_features], dim=1)\n        return self.fusion(combined)\n\nmultimodal_preds = np.zeros(len(test_df))\noof_multimodal = np.zeros(len(train_df))\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(train_df), 1):\n    print(f\"\\nMultimodal Fold {fold}/{Config.N_FOLDS}\")\n    \n    train_dataset = MultimodalDataset(\n        train_df['catalog_content'].iloc[train_idx].values,\n        train_image_features[train_idx],\n        train_df['log_price'].iloc[train_idx].values,\n        tokenizer, 128\n    )\n    val_dataset = MultimodalDataset(\n        train_df['catalog_content'].iloc[val_idx].values,\n        train_image_features[val_idx],\n        train_df['log_price'].iloc[val_idx].values,\n        tokenizer, 128\n    )\n    \n    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n    \n    model = MultimodalModel(image_feat_dim=train_image_features.shape[1]).to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n    criterion = nn.HuberLoss()\n    \n    best_loss = float('inf')\n    patience_counter = 0\n    \n    for epoch in range(4):\n        model.train()\n        for batch in train_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            image_features = batch['image_features'].to(device)\n            prices = batch['price'].to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(input_ids, attention_mask, image_features).squeeze()\n            loss = criterion(outputs, prices)\n            loss.backward()\n            optimizer.step()\n        \n        model.eval()\n        val_loss = 0\n        val_preds = []\n        with torch.no_grad():\n            for batch in val_loader:\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                image_features = batch['image_features'].to(device)\n                prices = batch['price'].to(device)\n                outputs = model(input_ids, attention_mask, image_features).squeeze()\n                loss = criterion(outputs, prices)\n                val_loss += loss.item()\n                val_preds.extend(outputs.cpu().numpy())\n        \n        avg_val_loss = val_loss / len(val_loader)\n        if avg_val_loss < best_loss:\n            best_loss = avg_val_loss\n            patience_counter = 0\n            oof_multimodal[val_idx] = val_preds\n        else:\n            patience_counter += 1\n            if patience_counter >= 2:\n                break\n    \n    # Test predictions\n    test_dataset = MultimodalDataset(\n        test_df['catalog_content'].values, test_image_features, None, tokenizer, 128\n    )\n    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n    \n    model.eval()\n    fold_test_preds = []\n    with torch.no_grad():\n        for batch in test_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            image_features = batch['image_features'].to(device)\n            outputs = model(input_ids, attention_mask, image_features).squeeze()\n            fold_test_preds.extend(outputs.cpu().numpy())\n    \n    multimodal_preds += np.array(fold_test_preds) / Config.N_FOLDS\n    \n    del model\n    torch.cuda.empty_cache()\n    gc.collect()\n\noof_smape_mm = smape(train_df['price'], np.expm1(oof_multimodal))\nprint(f\"\\nüèÜ Multimodal OOF SMAPE: {oof_smape_mm:.2f}\")\n\n# ============================================================================\n# CELL 15: Pseudo-Labeling (Use test predictions to retrain)\n# ============================================================================\nprint(\"\\nüîÑ Applying Pseudo-Labeling...\")\n\n# Get average predictions for test set\ntest_avg_preds = (\n    Config.CATBOOST_WEIGHT * catboost_preds +\n    Config.LGBM_WEIGHT * lgbm_preds +\n    Config.RIDGE_WEIGHT * ridge_preds +\n    Config.NN_TEXT_WEIGHT * nn_preds +\n    Config.MULTIMODAL_WEIGHT * multimodal_preds\n)\n\n# Select high-confidence predictions (middle 50% of predictions)\nq25 = np.percentile(test_avg_preds, 25)\nq75 = np.percentile(test_avg_preds, 75)\nconfident_mask = (test_avg_preds >= q25) & (test_avg_preds <= q75)\nconfident_indices = np.where(confident_mask)[0]\n\nprint(f\"Selected {len(confident_indices)} confident pseudo-labels\")\n\n# Create pseudo-labeled dataset\npseudo_features = test_features.iloc[confident_indices].copy()\npseudo_labels = test_avg_preds[confident_indices]\n\n# Combine with training data\ncombined_features = pd.concat([train_features, pseudo_features], ignore_index=True)\ncombined_labels = np.concatenate([train_df['log_price'].values, pseudo_labels])\n\n# Retrain CatBoost on combined data\nprint(\"Retraining CatBoost with pseudo-labels...\")\n\ncatboost_pseudo_preds = np.zeros(len(test_df))\n\nfor fold in range(2):  # 2 iterations for time\n    model = cb.CatBoostRegressor(\n        iterations=2000,\n        learning_rate=0.02,\n        depth=8,\n        random_seed=Config.SEED + fold,\n        verbose=0,\n        task_type='GPU' if torch.cuda.is_available() else 'CPU'\n    )\n    \n    model.fit(\n        combined_features, combined_labels,\n        cat_features=cat_feature_indices,\n        verbose=False\n    )\n    \n    catboost_pseudo_preds += model.predict(test_features) / 2\n    \n    del model\n    gc.collect()\n\nprint(\"‚úÖ Pseudo-labeling complete!\")\n\n# ============================================================================\n# CELL 16: Stacking Meta-Model\n# ============================================================================\nprint(\"\\nüé≠ Training Stacking Meta-Model...\")\n\n# Create meta-features (OOF predictions from base models)\nmeta_train = np.column_stack([\n    oof_catboost,\n    oof_lgbm,\n    oof_ridge,\n    oof_nn,\n    oof_multimodal\n])\n\nmeta_test = np.column_stack([\n    catboost_preds,\n    lgbm_preds,\n    ridge_preds,\n    nn_preds,\n    multimodal_preds\n])\n\n# Add some base features\nmeta_train_full = np.column_stack([\n    meta_train,\n    train_features[['pack_quantity', 'extracted_value', 'log_pack_quantity']].values\n])\n\nmeta_test_full = np.column_stack([\n    meta_test,\n    test_features[['pack_quantity', 'extracted_value', 'log_pack_quantity']].values\n])\n\n# Train Ridge meta-model\nstacking_preds = np.zeros(len(test_df))\noof_stacking = np.zeros(len(train_df))\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(train_df), 1):\n    X_tr = meta_train_full[train_idx]\n    y_tr = train_df['log_price'].iloc[train_idx]\n    X_val = meta_train_full[val_idx]\n    \n    meta_model = Ridge(alpha=5.0)\n    meta_model.fit(X_tr, y_tr)\n    \n    oof_stacking[val_idx] = meta_model.predict(X_val)\n    stacking_preds += meta_model.predict(meta_test_full) / Config.N_FOLDS\n\noof_smape_stack = smape(train_df['price'], np.expm1(oof_stacking))\nprint(f\"üèÜ Stacking OOF SMAPE: {oof_smape_stack:.2f}\")\n\n# ============================================================================\n# CELL 17: Final Ensemble\n# ============================================================================\nprint(\"\\n\" + \"=\"*70)\nprint(\"üéØ FINAL ENSEMBLE\")\nprint(\"=\"*70)\n\n# Weighted ensemble\nfinal_preds_log = (\n    Config.CATBOOST_WEIGHT * catboost_preds +\n    Config.LGBM_WEIGHT * lgbm_preds +\n    Config.RIDGE_WEIGHT * ridge_preds +\n    Config.NN_TEXT_WEIGHT * nn_preds +\n    Config.MULTIMODAL_WEIGHT * multimodal_preds +\n    Config.STACKING_WEIGHT * stacking_preds\n)\n\n# Add pseudo-labeled CatBoost with small weight\nfinal_preds_log = 0.92 * final_preds_log + 0.08 * catboost_pseudo_preds\n\n# Convert to actual prices\nfinal_preds = np.expm1(final_preds_log)\n\n# Calculate OOF ensemble\noof_ensemble_log = (\n    Config.CATBOOST_WEIGHT * oof_catboost +\n    Config.LGBM_WEIGHT * oof_lgbm +\n    Config.RIDGE_WEIGHT * oof_ridge +\n    Config.NN_TEXT_WEIGHT * oof_nn +\n    Config.MULTIMODAL_WEIGHT * oof_multimodal +\n    Config.STACKING_WEIGHT * oof_stacking\n)\n\noof_ensemble = np.expm1(oof_ensemble_log)\nfinal_oof_smape = smape(train_df['price'], oof_ensemble)\n\n# Print results\nprint(f\"\\nüìä Individual Model Performance:\")\nprint(f\"   CatBoost:      {oof_smape_cb:.2f} (weight: {Config.CATBOOST_WEIGHT})\")\nprint(f\"   LightGBM:      {oof_smape_lgb:.2f} (weight: {Config.LGBM_WEIGHT})\")\nprint(f\"   Ridge:         {oof_smape_ridge:.2f} (weight: {Config.RIDGE_WEIGHT})\")\nprint(f\"   Neural Net:    {oof_smape_nn:.2f} (weight: {Config.NN_TEXT_WEIGHT})\")\nprint(f\"   Multimodal:    {oof_smape_mm:.2f} (weight: {Config.MULTIMODAL_WEIGHT})\")\nprint(f\"   Stacking:      {oof_smape_stack:.2f} (weight: {Config.STACKING_WEIGHT})\")\nprint(f\"\\n{'='*70}\")\nprint(f\"üèÜ FINAL ENSEMBLE OOF SMAPE: {final_oof_smape:.2f}\")\nprint(f\"{'='*70}\")\n\n# ============================================================================\n# CELL 18: Post-Processing & Calibration\n# ============================================================================\nprint(\"\\nüîß Post-processing...\")\n\n# Ensure positive predictions\nfinal_preds = np.maximum(final_preds, 0.1)\n\n# Gentle calibration\ntrain_median = train_df['price'].median()\npred_median = np.median(final_preds)\ncalibration_factor = train_median / pred_median\n\n# Apply gentle calibration\ncalibrated_preds = final_preds * (0.8 + 0.2 * calibration_factor)\n\n# Clip extremes\nq01, q99 = train_df['price'].quantile([0.01, 0.99])\ncalibrated_preds = np.clip(calibrated_preds, q01 * 0.3, q99 * 2.0)\n\nprint(f\"Calibration factor: {calibration_factor:.3f}\")\nprint(f\"Price range: ${calibrated_preds.min():.2f} - ${calibrated_preds.max():.2f}\")\n\n# ============================================================================\n# CELL 19: Create Submissions\n# ============================================================================\nprint(\"\\nüìù Creating submissions...\")\n\n# Main submission\nsubmission = pd.DataFrame({\n    'sample_id': test_df['sample_id'],\n    'price': final_preds\n})\n\n# Calibrated submission\nsubmission_calibrated = pd.DataFrame({\n    'sample_id': test_df['sample_id'],\n    'price': calibrated_preds\n})\n\n# Verify\nassert len(submission) == len(test_df)\nassert (submission['price'] > 0).all()\n\n# Save\nsubmission.to_csv('submission.csv', index=False)\nsubmission_calibrated.to_csv('submission_calibrated.csv', index=False)\n\nprint(\"\\n‚úÖ Submissions saved!\")\nprint(f\"   - submission.csv\")\nprint(f\"   - submission_calibrated.csv\")\n\n\n  ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}